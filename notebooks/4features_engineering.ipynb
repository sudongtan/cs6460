{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: https://github.com/awarebayes/RecNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google's BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "import torch\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "# from tqdm import tqdm_notebook as tqdm\n",
    "# import torch\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "%matplotlib inline\n",
    "cuda = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model imports\n",
    "cuda = torch.device('cuda')\n",
    "bert = BertModel.from_pretrained('bert-base-uncased').to(cuda)\n",
    "bert.eval()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.read_csv('../data_raw/books_cleaned.csv')['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infos_tensor = {}\n",
    "for i, summary in tqdm(enumerate(text)):\n",
    "    summary = str(summary)[:512]\n",
    "    v = tokenizer.tokenize(summary)\n",
    "    v = tokenizer.convert_tokens_to_ids(v)\n",
    "    v = torch.tensor(v).to(cuda)\n",
    "    infos_tensor[i] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infos_sorted = OrderedDict(sorted(infos_tensor.items(), key=lambda t: t[1].size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infos_bert = {}\n",
    "batch = []\n",
    "indexes = []\n",
    "max_size = 0\n",
    "batch_size = 1\n",
    "\n",
    "for i in tqdm(range(len(infos_sorted))):\n",
    "    idx, tensor = infos_sorted.popitem()\n",
    "    batch.append(tensor)\n",
    "    indexes.append(idx)\n",
    "    \n",
    "    if len(batch) >= batch_size:\n",
    "        seq_lengths = torch.tensor([len(seq) for seq in batch]).long().cuda()\n",
    "        seq_tensor = torch.zeros((len(batch), seq_lengths.max())).long().cuda()\n",
    "        \n",
    "        for idx, (seq, seqlen) in enumerate(zip(batch, seq_lengths)):\n",
    "            seq_tensor[idx, :seqlen] = torch.tensor(seq).long().cuda()\n",
    "            \n",
    "        _, output = bert(seq_tensor)\n",
    "\n",
    "        output = output.detach().cpu()\n",
    "        for i in range(output.size(0)):\n",
    "            infos_bert[indexes[i]] = output[i]\n",
    "            \n",
    "        batch = []\n",
    "        indexes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_lengths = torch.tensor([len(seq) for seq in batch]).long().cuda()\n",
    "seq_tensor = torch.zeros((len(batch), seq_lengths.max())).long().cuda()\n",
    "\n",
    "for idx, (seq, seqlen) in enumerate(zip(batch, seq_lengths)):\n",
    "    seq_tensor[idx, :seqlen] = torch.tensor(seq).long().cuda()\n",
    "            \n",
    "    _, output = bert(seq_tensor)\n",
    "\n",
    "    output = output.detach().cpu()\n",
    "    for i in range(output.size(0)):\n",
    "        infos_bert[indexes[i]] = output[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(dict([(i[0], i[1].numpy()) for i in infos_bert.items()]),\n",
    "            open('../data_raw/books_bert.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = pickle.load(open('../data_raw/books_bert.p', 'rb'))\n",
    "embs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multicat_label_encode(infos, col, trim=1):\n",
    "    col_sort = []\n",
    "    [col_sort.extend(i[1:-1].split(',')) for i in infos[col]]\n",
    "    col_sort = pd.Series(col_sort).value_counts()\n",
    "    result = {}\n",
    "    valid = 0\n",
    "    for key, value in col_sort.items(): \n",
    "        if value > trim:\n",
    "            result[key] = value\n",
    "            valid += value\n",
    "    col_sort = pd.Series(result).index.tolist()\n",
    "    print(col, 'unique:', len(col_sort)) #, 'valid: ', valid)\n",
    "        \n",
    "    col_dict = dict([(k, idx) for idx, k in enumerate(col_sort)])\n",
    "    \n",
    "    def func(values):\n",
    "        values = values[1:-1].split(',')\n",
    "        result = []\n",
    "        for value in values:\n",
    "            standard = col_dict.get(value, len(col_sort))\n",
    "            result.append(standard)\n",
    "        return list(set(result))\n",
    "    \n",
    "    infos[col] = infos[col].apply(lambda x:func(x))\n",
    "#     print('have the non-label', sum(infos[col].apply(lambda x:any([i==len(col_sort) for i in x]))))\n",
    "#     print('only the non-label', sum(infos[col].apply(lambda x:len(x) ==0 and x[0] == len(col_sort))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_label_encode(infos, col, trim=1):\n",
    "    col_sort = infos[col].value_counts()\n",
    "    \n",
    "    result = {}\n",
    "    valid = 0\n",
    "    for key, value in col_sort.items(): \n",
    "        if value > trim:\n",
    "            result[key] = value\n",
    "            valid += value\n",
    "    col_sort = pd.Series(result).index.tolist()\n",
    "    print(col, 'unique:', len(col_sort)) #, 'valid: ', valid)\n",
    "    \n",
    "    \n",
    "    col_dict = dict([(k, idx) for idx, k in enumerate(col_sort)])\n",
    "    \n",
    "    infos[col] = infos[col].apply(lambda x:col_dict.get(x, len(col_sort)))\n",
    "#     print('have the non-label', sum(infos[col].apply(lambda x:x==len(col_sort))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books  = pd.read_csv('../data_raw/books_cleaned.csv')\n",
    "multicat_label_encode(books, 'authors', 5)\n",
    "multicat_label_encode(books, 'similar_books', 25)\n",
    "single_label_encode(books, 'publisher', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_year(year):\n",
    "    if year >= 2010:\n",
    "        return 0\n",
    "    elif year >= 2000:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "books['publication_year'] = books['publication_year'].apply(convert_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books['publication_year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standartization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(infos, col, plot=True, bins=100):\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    a = np.array(infos[col])\n",
    "\n",
    "    a = a.reshape(-1, 1)\n",
    "    #print(a.shape)\n",
    "    scaler = scaler.fit(a)\n",
    "    \n",
    "    def func(value):\n",
    "        value = np.array(value).reshape(-1, 1)\n",
    "        return scaler.transform(value)[0][0]\n",
    "        \n",
    "    \n",
    "    infos[col] = infos[col].apply(lambda x:func(x))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardize(books, 'ratings_count')\n",
    "standardize(books, 'average_rating')\n",
    "standardize(books, 'num_pages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = pickle.load(open('../data_raw/books_bert.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dict = {}\n",
    "\n",
    "for i in tqdm(texts.keys()):\n",
    "    \n",
    "    \n",
    "    similar = torch.zeros(1, 504+1)\n",
    "    authors = torch.zeros(1, 466+1)\n",
    "    publisher = torch.zeros(1, 513+1)\n",
    "\n",
    "    similar[0][[books['similar_books'].iloc[i]]] = 1\n",
    "    authors[0][[books['authors'].iloc[i]]] = 1\n",
    "    publisher[0][[books['publisher'].iloc[i]]] = 1\n",
    "\n",
    "\n",
    "    misc = torch.tensor([[books['num_pages'].iloc[i],\n",
    "                          books['ratings_count'].iloc[i],\n",
    "                          books['average_rating'].iloc[i]\n",
    "                       ]])\n",
    "    \n",
    "    text = torch.from_numpy(texts[i])\n",
    "    text = text.unsqueeze(0)\n",
    "    item = torch.cat([similar, authors, publisher, misc, text], 1)\n",
    "    \n",
    "    # note may produce some nans!\n",
    "    item[torch.isnan(item)] = 0\n",
    "    tensor_dict[i] = item.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tensor_dict,  open(\"../data_raw/books_embs_raw_dict.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_embs = pickle.load(open('../data_raw/books_embs_raw_dict.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dim(x):\n",
    "    pca = PCA(x)\n",
    "    data = torch.stack(list(books_embs.values()))\n",
    "    data[torch.isnan(data)] = 0\n",
    "    pca.fit(data.cpu().numpy())\n",
    "\n",
    "    dim_reduced = {}\n",
    "\n",
    "    for k,v in tqdm(books_embs.items()):\n",
    "        v = v.numpy()\n",
    "        v[np.isnan(v)] = 0\n",
    "        dim_reduced[k] = torch.from_numpy(pca.transform(v.reshape(1, -1))).squeeze()\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(-0.01, 0.01))\n",
    "    scaler.fit(np.stack(list(dim_reduced.values())))\n",
    "\n",
    "    for k,v in tqdm(dim_reduced.items()):\n",
    "        v = v.numpy()\n",
    "        dim_reduced[k] = scaler.transform(v.reshape(1, -1)).squeeze()\n",
    "        \n",
    "    #pickle.dump(dim_reduced,  open(f\"data/books_embs_{x}_dict.p\", \"wb\"))\n",
    "    \n",
    "    array = []\n",
    "    n = len(dim_reduced)\n",
    "    for i in range(n):\n",
    "        array.append(dim_reduced[i])\n",
    "    np.save(f'../data/books_embs_{x}.npy', np.array(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_dim(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_dim(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_dim(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_dim(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
